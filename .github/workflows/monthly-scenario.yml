name: Monthly Scenario Creation

on:
  workflow_dispatch:
    inputs:
      year:
        description: 'Year for the scenario (YYYY)'
        required: false
        default: ''
      month:
        description: 'Month for the scenario (MM)'
        required: false
        default: ''
  schedule:
    - cron: "0 3 1 * *" # Runs monthly on the 1st at 3:00 AM UTC

jobs:
  create-scenario:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v3

      - name: Install Python Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install openai tiktoken

      - name: Determine Year and Month
        id: determine_date
        run: |
          YEAR=${{ github.event.inputs.year }}
          MONTH=${{ github.event.inputs.month }}

          if [ -z "$YEAR" ] || [ -z "$MONTH" ]; then
            LAST_MONTH_DATE=$(date -d "last month" "+%Y-%m")
            YEAR=${YEAR:-${LAST_MONTH_DATE%-*}}
            MONTH=${MONTH:-${LAST_MONTH_DATE#*-}}
          fi

          echo "YEAR=$YEAR" >> $GITHUB_ENV
          echo "MONTH=$MONTH" >> $GITHUB_ENV
          echo "Determined year and month: $YEAR-$MONTH"

      - name: Set Input and Output Paths
        run: |
          INPUT_FILE="monthly-summaries/${{ env.YEAR }}-${{ env.MONTH }}-summary.txt"
          OUTPUT_FILE="monthly-scenario/${{ env.YEAR }}-${{ env.MONTH }}-scenario.txt"
          echo "INPUT_FILE=$INPUT_FILE" >> $GITHUB_ENV
          echo "OUTPUT_FILE=$OUTPUT_FILE" >> $GITHUB_ENV

      - name: Check for Monthly Summary
        run: |
          if [ ! -f "$INPUT_FILE" ]; then
            echo "Error: Monthly summary file $INPUT_FILE does not exist."
            exit 1
          fi

      - name: Reduce Token Count Before API Call
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          python - <<EOF
          import openai
          import tiktoken

          openai.api_key = "${{ secrets.OPENAI_API_KEY }}"
          enc = tiktoken.encoding_for_model("gpt-4")
          SYSTEM_MESSAGE_TOKENS = 50
          MAX_COMPLETION_TOKENS = 800
          MAX_TOTAL_TOKENS = 8192
          MAX_INPUT_TOKENS = MAX_TOTAL_TOKENS - SYSTEM_MESSAGE_TOKENS - MAX_COMPLETION_TOKENS

          # Read input file
          with open("$INPUT_FILE", "r", encoding="utf-8") as f:
              summary = f.read()

          # Tokenize and count
          tokens = enc.encode(summary)
          token_count = len(tokens)

          print(f"Original Token Count: {token_count}")
          print(f"Maximum Allowed Input Tokens: {MAX_INPUT_TOKENS}")

          if token_count > MAX_INPUT_TOKENS:
              print("Input exceeds token limit. Reducing content...")

              # Split the summary into smaller chunks
              chunk_size = MAX_INPUT_TOKENS // 3  # Split into 3 parts to avoid limit
              chunks = [summary[i:i+chunk_size] for i in range(0, len(summary), chunk_size)]
              
              summarized_chunks = []

              for chunk in chunks:
                  print("Summarizing a chunk...")

                  response = openai.chat.completions.create(
                      model="gpt-4",
                      messages=[
                          {"role": "system", "content": "Summarize the following text into key sentences while keeping necessary details for a dialogue scenario. Ensure that no sentence is overly long."},
                          {"role": "user", "content": chunk}
                      ],
                      max_tokens=chunk_size // 2,  # Reduce output length per chunk
                      temperature=0.7
                  )

                  summarized_chunks.append(response.choices[0].message.content)

              summary = " ".join(summarized_chunks)
              print(f"Reduced token count: {len(enc.encode(summary))}")

          # Save processed summary
          with open("processed_summary.txt", "w", encoding="utf-8") as f:
              f.write(summary)
          EOF

      - name: Generate Scenario Using OpenAI API (Non-Streaming)
        id: generate_scenario
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          # Read reduced summary
          SUMMARY=$(cat processed_summary.txt)

          # Create API payload (non-streaming)
          PAYLOAD=$(jq -n --arg summary "$SUMMARY" '{
            model: "gpt-4",
            messages: [
              {
                role: "system",
                content: "Generate a realistic dialogue scenario. Output: 1. Japanese script, 2. Romaji, 3. English translation. Keep under 800 tokens."
              },
              {
                role: "user",
                content: $summary
              }
            ],
            max_tokens: 800
          }')

          echo "Sending request to OpenAI API (non-streaming)..."

          # Make API request
          RESPONSE=$(curl -s https://api.openai.com/v1/chat/completions \
            -H "Authorization: Bearer $OPENAI_API_KEY" \
            -H "Content-Type: application/json" \
            -d "$PAYLOAD")

          # Extract scenario content
          SCENARIO=$(echo "$RESPONSE" | jq -r '.choices[0].message.content // "Error: Unable to process scenario."')

          # Ensure response is not empty
          if [[ "$SCENARIO" == "Error"* ]]; then
            echo "Error during scenario generation: $SCENARIO"
            exit 1
          fi

          # Save final output
          echo -e "$SCENARIO" > "$OUTPUT_FILE"

      - name: Commit and Push Scenario to Repo
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          git config user.name "GitHub Actions"
          git config user.email "actions@github.com"

          mkdir -p $(dirname "$OUTPUT_FILE")
          git add "$OUTPUT_FILE"
          git commit -m "Add monthly scenario for ${{ env.YEAR }}-${{ env.MONTH }}"
          git push

      - name: Upload Scenario as Artifact
        uses: actions/upload-artifact@v4
        with:
          name: Monthly-Scenario-${{ env.YEAR }}-${{ env.MONTH }}
          path: ${{ env.OUTPUT_FILE }}
